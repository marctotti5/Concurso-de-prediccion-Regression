---
title: Regression Models Competition
author: Outliers
number-sections: true
editor:
  render-on-save: true
---

## Executive Summary

This report outlines the development of a predictive model for residential home
prices in Ames, Iowa. The objective was to minimize the Root Mean Squared Error
(RMSE) between the predicted logarithm of the sale price and the actual values.

Our analysis proceeded through data cleaning, feature engineering, and the
implementation of regularized linear regression models (Ridge, Lasso, and
ElasticNet). The final selected approach is an ensemble of these three models,
which provides robust predictions by balancing bias and variance. Key drivers of
price were identified as the total square footage, overall quality, and
neighborhood location.

## Introduction & Objectives

The goal of this project is to predict the final sale price of homes based on 79
explanatory variables describing (almost) every aspect of residential homes.
Anything studied during the course could be used to create the models, but more
advanced techniques were out of the scope.

The specific objectives were:

1. Perform Exploratory Data Analysis (EDA) to understand variable distributions
   and relationships.
2. Clean the data by handling missing values and outliers.
3. Engineer new features to capture latent information.
4. Train and validate predictive models using cross-validation.
5. Identify the characteristics of the most and least expensive houses.

## Exploratory Data Analysis (EDA)

### Preliminary steps

Right away from reading the data, we imputed the missing categories as the feature being missing. In @sec-imputation-strategy we deglose the process in more detail, but it was important to mention this beforehand.

In the same spirit, since we were clear that many of the available variables were correlated and redundant, we created a handful of interest variables from the beginning to include them in the EDA (refer to @sec-feature-engineer for a detailed explanation them).


### Analysis of the Target Variable (`SalePrice`)

We began by analyzing the distribution of the dependent variable, `SalePrice`.
The initial histogram (see @fig-sale-price) revealed a significant right skew,
indicating that while most homes are moderately priced, there are a few very
expensive properties that stretch the distribution.

:::{#fig-sale-price layout-ncol=2}
![Histogram of SalePrice](images/LogSalePrice-p1.svg)

![Q-Q plot of SalePrice](images/LogSalePrice-p2.svg)

![Histogram of log(SalePrice)](images/LogSalePrice-p3.svg)

![Q-Q plot of log(SalePrice)](images/LogSalePrice-p4.svg)


Distribution analysis on the response variable. Original is left-skewed, whereas the log-transformed resembles a normal distribution.
:::

To address this non-normality and satisfy the assumptions of linear regression,
we applied a logarithmic transformation (`log(SalePrice)`). This resulted in a
distribution that much closely approximates a normal curve, as confirmed by the
Q-Q plots generated during analysis (again, see @fig-sale-price).

### Correlation Analysis

We examined the correlation between numerical predictors and the sale price. The
variables with the highest positive correlation were `TotalSF` (aggregation of the existing area-related variables), `TotalBath` (weighted-aggregation of the bathrooms and half bathrooms of the house), and `GarageCars` (size of the garage measured in cars). As an important mark, we used Spearman's rank correlation coefficient, since it is not affected by monotonic transformations such as the logarithm. See @fig-top-correlation for more details.

:::{#fig-top-correlation}
![](images/top-correlation.svg)

The ten highest correlated variables with the `SalePrice` using Spearman's rank correlation coefficient.
:::

We also investigated categorical variables. Boxplots of `SalePrice` against
`Neighborhood` showed significant variation in median prices, confirming that
location is a critical determinant of value. Similarly, `OverallQual` showed a
clear monotonic relationship with price. Interestingly, during our analysis, we also saw that `ExterQual` and `KitchenQual` are strongly related to an increment in price. See @fig-categorical for more details.

::: {#fig-categorical}
![](images/corr_cat.svg)

Boxplots of several categorical variables against the sale price. Overall quality, neighborhood, exterior-material quality and kitchen quality are all related to an increase in price, each of them in different levels.
:::

### Missing Values Analysis

A detailed analysis of missing data revealed two distinct types of missingness:

1. **Structural Missingness:** Variables like `Alley`, `PoolQC`, and `Fence` had
   high rates of missing values. However, the data dictionary implies that `NA`
   here effectively means "Feature Not Present" (e.g., No Alley access) rather
   than missing data. Therefore, these values were correctly imputed when reading the data.
2. **Random Missingness:** Variables like `LotFrontage` showed missingness
   patterns that required statistical imputation, since the median of the observations without value is higher than those with it (see @fig-missing-impact). The other variables with missing values were `Electrical`, `MasVnrArea` and `MasVnrType`. These last two were an interesting case, since both were missing (and present) at the same time in each row, suggesting both measurements can be taken (or not) at the same time.

:::{#fig-missing-impact}
![](images/missing_lotfrontage_impact.svg)

Boxplot of the distribution of prices (in $log$ scale) grouped by the values of `LotFrontage` being missing or not.
There is a non-trivial difference between both groups.
:::

## Data Preprocessing & Feature Engineering

### Outlier Detection and Influence Analysis {#sec-outliers}

Before proceeding with imputation, we performed a diagnostic check to identify observations that could disproportionately bias our model's parameters. We focused on influential points, which are observations that possess both high leverage and high residuals.

Using a preliminary linear model, we calculated Cook's Distance for every observation in the training set. We applied the standard threshold of $D_i > 4/(n - k - 1)$ to flag potential problems. 

:::{#fig-influence-diag}
![](images/influential.svg)

Diagnostic plots for identifying influential points. The left panel shows Standardized Residuals vs. Leverage with a Cook's Distance threshold of $D_i > 4/(n-k-1)$. The right panel demonstrates how removing IDs 1299 and 524 (dashed green line) improves the regression trend for the rest of the market.
:::

As shown in @fig-influence-diag, two specific observations (IDs 1299 and 524) were identified as extreme outliers. These properties feature total square footage exceeding 4,000 sq ft but sold for prices significantly below the expected trend for their size. Including these points would "pull" the regression line downward, leading to an underestimation of value for other large properties. Consequently, these two observations were removed to ensure model robustness.


### Imputation Strategy {#sec-imputation-strategy}

Based on the EDA, we implemented a two-tiered imputation strategy:

- **Categorical Encoding:** For variables like `PoolQC`, `GarageType`, and
  `BsmtQual`, `NA` values were recoded to a explicit category (e.g., "No Pool",
  "No Garage") to preserve the information that the feature is missing. This was instructed in the problem statement, so this step was performed at the beginning, when reading the data. Note as well that variables such as `OverallQual` were converted into categorical, but this was not entirely required since it could have been used as a numerical predictor as well.
- **Statistical Imputation:** Remaining missing values in numerical columns were
  handled using statistical methods to ensure the model could utilize all available records. Specifically, they were imputed using the MICE algorithm (Multivariate Imputation by Chained-Equations) with Random
  Forest. To ensure robustness and prevent data leakage, the imputation
  model was trained exclusively on the training data while applying the learned patterns to complete the test set.

### Feature Engineering {#sec-feature-engineer}

To improve model interpretability and performance, we created several composite
variables:

- **`TotalSF` (Total Square Footage):** We combined `TotalBsmtSF` and
  `GrLivArea` to create a single metric for the size of the house. This variable
  proved to have a higher correlation with price than its individual
  components.
- **`TotalBath`:** We aggregated full and half baths (both above ground and
  basement) into a single count.
- **`HouseAge` & `YearsSinceRemod`:** We converted `YearBuilt` and
  `YearRemodAdd` into age variables relative to the year sold, which captures
  depreciation more effectively than raw years.
- **`TotalPorchSF`:** Summed the area of all decks and porches.

### Variable Selection

To reduce noise and multicollinearity, we performed the following cleaning
steps:

- **Near Zero Variance (NZV) Removal:** Variables with almost no variation
  (e.g., `Utilities` where 99.9% were "AllPub", `Street`, and `PoolArea`) were
  removed as they provide no predictive power.
- **Redundancy Removal:** Highly correlated features were removed. For instance,
  `GarageArea` was removed in favor of `GarageCars`, and `TotalBsmtSF` was
  removed as it is now included in `TotalSF`. `FullBath` in favor of `TotalBath`, etc.
  
Note that for categorical variables a two-step feature selection was performed.
First, we calculated a Cramer’s V matrix to detect multicollinearity. Second,
we applied the Kruskal-Wallis test against \texttt{SalePrice}
($\log$-transformed) to evaluate feature importance, identifying and flagging
variables that showed no statistically significant relationship ($p > 0.05$)
with the target.

Following this approach we remained with 44 predictors, which is still quite a large number. Moreover, since categorical variables are turned into dummy variables when creating the model, this number could grow rapidly.
For this reason, we used model selection techniques, as described in the following section.

## Methodology (Model Selection)

We tested several models and chose the best among them, some of them being **Regularized Linear Regression** models. Given the high dimensionality of the dataset (many categorical variables turned into dummy variables) and the presence of multicollinearity, ordinary least-squares regression is prone to overfitting. Regularization introduces a penalty term to shrink coefficients, improving generalization.

The tested models were:

1. **Ridge Regression:** Penalizes the sum of squared coefficients. It shrinks coefficients but keeps all variables.
1. **Lasso Regression:** Penalizes the sum of absolute values of coefficients. It can force coefficients to zero, effectively performing
   feature selection.
1. **ElasticNet:** A compromise between Ridge and Lasso.
1. **Principal Component Regression:** Instead of regressing on the predictors directly, the principal components are used as regressors.
1. **$k$-neighbors:**
1. **GAM (splines):**
1. **Linear with Forward Selection:**
1. **Linear with Backwards Selection:**

The spline approach was the most promising across the tested methods, producing the least RMSE (tested with cross-validation). PCR lost too much specific information in the components, and KNN struggled with the high dimensionality. None of the selection methods provided good metrics, so they were discarded.

In the end however, we decided to go with a combination of Lasso, Ridge Regression and ElasticNet, weighting the output of each of them to produce the final one.

### The Ensemble Approach

Rather than relying on a single model, we constructed an **Ensemble** by
averaging the predictions of the Ridge, Lasso, and ElasticNet models.  Ensembling
helps to cancel out the biases of individual models and typically yields a lower
RMSE than any single model on its own.

### Interaction selection

The models we have been building so far were only using predictors with no interactions between them. However, after some evaluation, we decided to use Lasso to select the best interactions among the possible, finding an interesting discovery in the process: quantity needs to be measured by quality.

Every significant interaction follows this pattern. For example, the interaction between `TotalSF` and `OverallQual` was very significant, or the `OverallQual` and the `TotalBath`.

## Results & Discussion

### Goodness of Fit

<!-- Aquí falta meter una buena discusión de los resultados, de las métricas que estamos usando y tal, de que suponemos que el error aumentará con el test porque siempre hay cierto overfitting, etc. -->


### Specific Questions Assessment

<!-- Esto hay que reescribirlo para que se quede un poco mejor, el punto es que estaría bien responder a las preguntas que la profesora plantea en el enunciado. -->

**Q1: What is the variable that contributes the most to explain the variability
in price?** Based on our correlation analysis and the magnitude of coefficients
in the Lasso model, **`TotalSF` (Total Square Footage)** is the single most
important predictor. It captures the size of the living area and basement, which
is the primary driver of cost. **`OverallQual`** is the second most critical
factor.

**Q2: What are the characteristics of the cheapest/most expensive houses?**

- **Most Expensive:** Characterized by high `OverallQual` (9-10), large
  `TotalSF` (>4000 sq ft), recent construction (`HouseAge` < 5 years), and
  location in neighborhoods like _NridgHt_ or _StoneBr_. They almost always
  possess a 3-car garage and high-quality basement finishes.
- **Cheapest:** Characterized by low `OverallQual` (1-4), lack of a garage
  (`GarageType` = "No Garage"), older age (`HouseAge` > 50 years) without
  remodeling, and often possess the `Grav` (Gravel) alley access type or belong
  to the _MeadowV_ neighborhood.

## Conclusion

<!-- Conclusiones por mejorar -->

By rigorously cleaning the data, engineering relevant features like `TotalSF`,
and utilizing an ensemble of regularized regression models, we created a robust
tool for predicting house prices. The model confirms that while size and quality
are paramount, the specific combination of location and amenities refines the
final valuation. Future work could explore tree-based models (like XGBoost) to
capture non-linear relationships that linear models might miss.
