---
title: Regression Models Competition
author: Outliers
number-sections: true
editor:
  render-on-save: true
---

## Executive Summary

This report outlines the development of a predictive model for residential home
prices in Ames, Iowa. The objective was to minimize the Root Mean Squared Error
(RMSE) between the predicted logarithm of the sale price and the actual values.

Our analysis proceeded through data cleaning, feature engineering, and the
implementation of regularized linear regression models (Ridge, Lasso, and
ElasticNet). The final selected approach is an ensemble of these three models,
which provides robust predictions by balancing bias and variance. Key drivers of
price were identified as the total square footage, overall quality, and
neighborhood location.

## Introduction & Objectives

The goal of this project is to predict the final sale price of homes based on 79
explanatory variables describing (almost) every aspect of residential homes.
Anything studied during the course could be used to create the models, but more
advanced techniques were out of the scope.

The specific objectives were:

1. Perform Exploratory Data Analysis (EDA) to understand variable distributions
   and relationships.
2. Clean the data by handling missing values and outliers.
3. Engineer new features to capture latent information.
4. Train and validate predictive models using cross-validation.
5. Identify the characteristics of the most and least expensive houses.

## Exploratory Data Analysis (EDA)

### Preliminary steps

Right away from reading the data, we imputed the missing categories as the feature being missing. In @sec-imputation-strategy we deglose the process in more detail, but it was important to mention this beforehand.

In the same spirit, since we were clear that many of the available variables were correlated and redundant, we created a handful of interest variables from the beginning to include them in the EDA (refer to @sec-feature-engineer for a detailed explanation them).


### Analysis of the Target Variable (`SalePrice`)

We began by analyzing the distribution of the dependent variable, `SalePrice`.
The initial histogram (see @fig-sale-price) revealed a significant right skew,
indicating that while most homes are moderately priced, there are a few very
expensive properties that stretch the distribution.

:::{#fig-sale-price layout-ncol=2}
![Histogram of SalePrice](images/LogSalePrice-p1.svg)

![Q-Q plot of SalePrice](images/LogSalePrice-p2.svg)

![Histogram of log(SalePrice)](images/LogSalePrice-p3.svg)

![Q-Q plot of log(SalePrice)](images/LogSalePrice-p4.svg)


Distribution analysis on the response variable. Original is left-skewed, whereas the log-transformed resembles a normal distribution.
:::

To address this non-normality and satisfy the assumptions of linear regression,
we applied a logarithmic transformation (`log(SalePrice)`). This resulted in a
distribution that much closely approximates a normal curve, as confirmed by the
Q-Q plots generated during analysis (again, see @fig-sale-price).

### Correlation Analysis

We examined the correlation between numerical predictors and the sale price. The
variables with the highest positive correlation were `TotalSF` (aggregation of the existing area-related variables), `TotalBath` (weighted-aggregation of the bathrooms and half bathrooms of the house), and `GarageCars` (size of the garage measured in cars). As an important mark, we used Spearman's rank correlation coefficient, since it is not affected by monotonic transformations such as the logarithm. See @fig-top-correlation for more details.

:::{#fig-top-correlation}
![](images/top-correlation.svg)

The ten highest correlated variables with the `SalePrice` using Spearman's rank correlation coefficient.
:::

We also investigated categorical variables. Boxplots of `SalePrice` against
`Neighborhood` showed significant variation in median prices, confirming that
location is a critical determinant of value. Similarly, `OverallQual` showed a
clear monotonic relationship with price. Interestingly, during our analysis, we also saw that `ExterQual` and `KitchenQual` are strongly related to an increment in price. See @fig-categorical for more details.

::: {#fig-categorical}
![](images/corr_cat.svg)

Boxplots of several categorical variables against the sale price. Overall quality, neighborhood, exterior-material quality and kitchen quality are all related to an increase in price, each of them in different levels.
:::

### Missing Values Analysis

A detailed analysis of missing data revealed two distinct types of missingness:

1. **Structural Missingness:** Variables like `Alley`, `PoolQC`, and `Fence` had
   high rates of missing values. However, the data dictionary implies that `NA`
   here effectively means "Feature Not Present" (e.g., No Alley access) rather
   than missing data. Therefore, these values were correctly imputed when reading the data.
2. **Random Missingness:** Variables like `LotFrontage` showed missingness
   patterns that required statistical imputation, since the median of the observations without value is higher than those with it (see @fig-missing-impact). The other variables with missing values were `Electrical`, `MasVnrArea` and `MasVnrType`. These last two were an interesting case, since both were missing (and present) at the same time in each row, suggesting both measurements can be taken (or not) at the same time.

:::{#fig-missing-impact}
![](images/missing_lotfrontage_impact.svg)

Boxplot of the distribution of prices (in $log$ scale) grouped by the values of `LotFrontage` being missing or not.
There is a non-trivial difference between both groups.
:::

## Data Preprocessing & Feature Engineering

### Imputation Strategy {#sec-imputation-strategy}

Based on the EDA, we implemented a two-tiered imputation strategy:

- **Categorical Encoding:** For variables like `PoolQC`, `GarageType`, and
  `BsmtQual`, `NA` values were recoded to a explicit category (e.g., "No Pool",
  "No Garage") to preserve the information that the feature is missing. This was instructed in the problem statement, so this step was performed at the beginning, when reading the data. Note as well that variables such as `OverallQual` were converted into categorical, but this was not entirely required since it could have been used as a numerical predictor as well.
- **Statistical Imputation:** Remaining missing values in numerical columns were
  handled using statistical methods to ensure the model could utilize all available records. Specifically, they were imputed using the MICE algorithm (Multivariate Imputation by Chained-Equations) with Random
  Forest. To ensure robustness and prevent data leakage, the imputation
  model was trained exclusively on the training data while applying the learned patterns to complete the test set.

### Feature Engineering {#sec-feature-engineer}

To improve model interpretability and performance, we created several composite
variables:

- **`TotalSF` (Total Square Footage):** We combined `TotalBsmtSF` and
  `GrLivArea` to create a single metric for the size of the house. This variable
  proved to have a higher correlation with price than its individual
  components.
- **`TotalBath`:** We aggregated full and half baths (both above ground and
  basement) into a single count.
- **`HouseAge` & `YearsSinceRemod`:** We converted `YearBuilt` and
  `YearRemodAdd` into age variables relative to the year sold, which captures
  depreciation more effectively than raw years.
- **`TotalPorchSF`:** Summed the area of all decks and porches.

### Variable Selection

To reduce noise and multicollinearity, we performed the following cleaning
steps:

- **Near Zero Variance (NZV) Removal:** Variables with almost no variation
  (e.g., `Utilities` where 99.9% were "AllPub", `Street`, and `PoolArea`) were
  removed as they provide no predictive power.
- **Redundancy Removal:** Highly correlated features were removed. For instance,
  `GarageArea` was removed in favor of `GarageCars`, and `TotalBsmtSF` was
  removed as it is now included in `TotalSF`. `FullBath` in favor of `TotalBath`, etc.
  
Note that for categorical variables a two-step feature selection was performed.
First, we calculated a Cramer’s V matrix to detect multicollinearity. Second,
we applied the Kruskal-Wallis test against \texttt{SalePrice}
($\log$-transformed) to evaluate feature importance, identifying and flagging
variables that showed no statistically significant relationship ($p > 0.05$)
with the target.

Following this approach we remained with 44 predictors, which is still quite a large number. Moreover, since categorical variables are turned into dummy variables when creating the model, this number could grow rapidly.
For this reason, we used model selection techniques, as described in the following section.

## Methodology and Model Selection

The model selection process evolved from simple linear regression toward advanced regularization and nonlinear modeling techniques. Our objective was not only to minimize error, but to capture the intrinsic complexity of the Ames housing market. We evaluated a full spectrum of algorithms, maintaining a rigorous comparison among them:

1. **Regularized Linear Models Ridge Lasso and Elastic Net:** Implemented to manage high dimensionality and multicollinearity by penalizing coefficients in order to improve generalization.
2. **Principal Components Regression PCR:** Used to reduce the dimensionality of the predictor space, at the cost of potential loss of interpretability.
3. **K Nearest Neighbors KNN:** A nonparametric approach based on local similarity between properties.
4. **Stepwise Variable Selection Forward and Backward:** Iterative methods based on information criteria AIC to identify the optimal subset of predictors.
5. **Generalized Additive Models GAM:** Our final proposal, based on the use of smoothing splines to model nonlinear relationships.

### Evaluation of Initial Candidates

After extensive cross validation, a clear performance hierarchy emerged. **Regularization models Lasso and Elastic Net** established a very solid benchmark, outperforming **stepwise selection methods**, which tended toward mild overfitting due to greedy decisions on individual variables.

By contrast, **PCR and KNN** showed the weakest performance. PCR, by constructing components without considering the response variable, diluted critical nuances for appraisal, while KNN suffered from the curse of dimensionality, where distance in a space with more than 80 variables loses predictive meaning.

### The Winning Model

Despite the robustness of linear models, we identified that the market does not behave in a purely additive and linear way. For this reason, we selected a **Generalized Additive Model GAM**, which was optimized through a process of statistical craftsmanship in three phases:

#### Nonlinearity Diagnosis and Splines

We conducted a linearity check by comparing linear fits against local smoothers LOESS. We confirmed that variables such as `TotalSF` and `HouseAge` exhibited curvature that a straight line could not capture.  
To optimize this, we implemented **cubic regression splines `bs='cr'`**, manually tuning the basis dimension **k** up to `k=40`. This increase in flexibility allowed the model to learn complex patterns, such as diminishing returns to built area, without fitting noise.

#### Interactions

One of the most powerful findings in our optimization phase was that the effect of quantity square meters or bathrooms is modulated by quality. We did not include interactions at random. Instead, we used **two way ANOVA** to identify the most significant pairs:

- **Size Quality Interaction:** We implemented a cross between `TotalSF` and `OverallQual`, capturing that an additional square meter in a luxury home is worth exponentially more than in a low quality home.
- **Condition Dependent Depreciation:** We introduced the term `s(HouseAge, by = OverallCond)`, allowing the aging curve of the property to differ according to its state of maintenance.

#### Robustness Through Random Effects and REML

To avoid overfitting in variables with many categories, such as `Neighborhood`, we treated neighborhood as a **random effect `bs='re'`**. This applies shrinkage that stabilizes predictions in areas with few observations.  
Finally, the model was fitted using **Restricted Maximum Likelihood REML**. Unlike other selection methods, REML provides a much more robust complexity penalty, ensuring that the model captures the true data structure rather than noise from the training set.

### Benchmarking Conclusion

The final results table confirms our hypothesis. The **GAM** outperforms linear regularization models by being able to bend where market reality demands it. While Elastic Net and Lasso provide an excellent linear approximation, the optimized GAM with quality driven interactions and neighborhood random effects stands out as the most accurate and balanced predictive tool for this competition.


## Results & Discussion

### Goodness of Fit

<!-- Aquí falta meter una buena discusión de los resultados, de las métricas que estamos usando y tal, de que suponemos que el error aumentará con el test porque siempre hay cierto overfitting, etc. -->


### Specific Questions Assessment

<!-- Esto hay que reescribirlo para que se quede un poco mejor, el punto es que estaría bien responder a las preguntas que la profesora plantea en el enunciado. -->

**Q1: What is the variable that contributes the most to explain the variability
in price?** Based on our correlation analysis and the magnitude of coefficients
in the Lasso model, **`TotalSF` (Total Square Footage)** is the single most
important predictor. It captures the size of the living area and basement, which
is the primary driver of cost. **`OverallQual`** is the second most critical
factor.

**Q2: What are the characteristics of the cheapest/most expensive houses?**

- **Most Expensive:** Characterized by high `OverallQual` (9-10), large
  `TotalSF` (>4000 sq ft), recent construction (`HouseAge` < 5 years), and
  location in neighborhoods like _NridgHt_ or _StoneBr_. They almost always
  possess a 3-car garage and high-quality basement finishes.
- **Cheapest:** Characterized by low `OverallQual` (1-4), lack of a garage
  (`GarageType` = "No Garage"), older age (`HouseAge` > 50 years) without
  remodeling, and often possess the `Grav` (Gravel) alley access type or belong
  to the _MeadowV_ neighborhood.

## Conclusion

<!-- Conclusiones por mejorar -->

By rigorously cleaning the data, engineering relevant features like `TotalSF`,
and utilizing an ensemble of regularized regression models, we created a robust
tool for predicting house prices. The model confirms that while size and quality
are paramount, the specific combination of location and amenities refines the
final valuation. Future work could explore tree-based models (like XGBoost) to
capture non-linear relationships that linear models might miss.
